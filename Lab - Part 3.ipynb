{
 "metadata": {
  "name": "Lab - Part 3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\nRun morphological analysis first (with medical lexicon)\n\"\"\"\n\nimport urllib2\nimport urllib\nimport codecs\nimport os\nimport sys\n\ndef do_post(text):\n    #params = urllib.urlencode({'mytext':text.encode(\"cp1255\"),\"output\":\"plain\"})\n    params = urllib.urlencode({'mytext':text,\"output\":\"plain\"})\n    url = \"http://www.cs.bgu.ac.il/~cohenrap/hebmed/\"\n    request = urllib2.Request(url, params)\n    response = urllib2.urlopen(request)\n    return response.read()\n\"\"\"\nSanity check!\n\"\"\"\nprint  do_post(\"\u05db\u05d5\u05d0\u05d1 \u05dc\u05d9 \u05d4\u05e8\u05d0\u05e9. \u05de\u05ea\u05d9 \u05e0\u05d2\u05de\u05e8\u05ea \u05d4\u05de\u05e2\u05d1\u05d3\u05d4?\")",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\u05db\u05d5\u05d0\u05d1\t:VERB-M,S,A,BEINONI:\t\u05db\u05d0\u05d1\tO\tpain-C0030193\n\u05dc\u05d9\t:PREPOSITION:PRONOMIAL-MF,S,1\t\u05dc\tB-NP\tO\n\u05d4\u05e8\u05d0\u05e9\tDEF:NOUN-M,S,ABS:\t\u05e8\u05d0\u05e9\tB-NP\thead-C0018670\n.\t:PUNCUATION:\t.\tO\tO\n\n\u05de\u05ea\u05d9\t:ADVERB:\t\u05de\u05ea\u05d9\tO\tO\n\u05e0\u05d2\u05de\u05e8\u05ea\t:VERB-F,S,A,BEINONI:\t\u05e0\u05d2\u05de\u05e8\tO\tO\n\u05d4\u05de\u05e2\u05d1\u05d3\u05d4\tDEF:NOUN-F,S,ABS:\t\u05de\u05e2\u05d1\u05d3\u05d4\tB-NP\tLaboratory-C0022877\n?\t:PUNCUATION:\t?\tO\tO\n\n\n"
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\nencoding=cp1255\nNavigate to the work folder\n\"\"\"\nimport os\nworkDir = r\"C:\\Users\\cohenr5\\Documents\\Syncplicity Folders\\academia\\medical hebrew\\lab\\classificationExample\"\nos.chdir(workDir)\n\nfilesDir = \"sample\"\n\n\"\"\"\nSanity Check\n\"\"\"\nprint os.listdir(filesDir)[0]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "0-lung\n"
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\nRead the texts\n\"\"\"\nfrom codecs import open\ndocs = {}\nfor f in os.listdir(filesDir):\n    fin = open(os.path.join(filesDir,f),\"r\",\"utf8\")\n    docs[f] = fin.read()\n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\ntag and annotate with the web service\n\"\"\"\ntaggedDocs = {}\nfor doc in docs.keys()[:5]\n    tagged = \"\"\n    print \"tagging\",doc\n    tagged= do_post(docs[doc].encode(\"utf8\"))\n    taggedDocs[doc] = tagged\n\n\"\"\"\nThis code snippet was used to create the canned annotated sample\ntDir = \"sampleTagged\"\nfor f in os.listdir(outDir):\n    fin = open(os.path.join(outDir,f),\"r\")\n    tagged = do_post(fin.read())\n    print f,len(tagged)\n\n    open(os.path.join(tDir,f),\"w\",\"utf8\").write(unicode(tagged,\"utf8\"))\n\"\"\"\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "tagging 170-lung\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 544-leukemia\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 426-leukemia\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 1397-leukemia\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 186-lung\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 213-lung\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 5-lung\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 408-lung\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 1980-leukemia\ntagging"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 2208-leukemia\n"
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\nSanity check!\n\"\"\"\nprint taggedDocs.values()[4]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\u05e8\u05e1\u05d9\u05d1\u05d4\t:PROPERNAME:\t\u05e8\u05e1\u05d9\u05d1\u05d4\tB-NP\tO\n\u05d9\u05db\u05d5\u05dc\u05d4\t:MODAL-F,S,A,BEINONI:\t\u05d9\u05db\u05d5\u05dc\tO\tO\n\u05dc\u05d2\u05e8\u05d5\u05dd\t:VERB-TOINFINITIVE:\t\u05d2\u05e8\u05dd\tO\tO\n\u05dc\u05d7\u05d5\u05dc\u05e9\u05d4\tPREPOSITION:NOUN-F,S,ABS:\t\u05d7\u05d5\u05dc\u05e9\u05d4\tB-NP\tAsthenia-C0004093\n\u05d0\u05dd\t:CONJUNCTION:\t\u05d0\u05dd\tO\tO\n\u05db\u05d9\t:CONJUNCTION:\t\u05db\u05d9\tO\tO\n\u05dc\u05e8\u05d5\u05d1\tPREPOSITION:NOUN-M,S,CONST:\t\u05e8\u05d5\u05d1\tB-NP\tmajority-C0680220\n\u05e4\u05d7\u05d5\u05ea\t:NOUN-M,P,ABS:\t\u05e4\u05d7\u05d4\tI-NP\tO\n\u05de\u05db\u05de\u05d5\u05ea\u05e8\u05e4\u05d9\u05d4\tPREPOSITION:NOUN-F,S,ABS:\t\u05db\u05de\u05d5\u05ea\u05e8\u05e4\u05d9\u05d4\tO\tchemotherapy-C0013216\n\u05d9\u05ea\u05db\u05df\t:MODAL:\t\u05d9\u05d9\u05ea\u05db\u05df\tO\tO\n\u05e9\u05d4\u05d7\u05d5\u05dc\u05e9\u05d4\tREL-SUBCONJ+DEF:NOUN-F,S,ABS:\t\u05d7\u05d5\u05dc\u05e9\u05d4\tB-NP\tAsthenia-C0004093\n\u05e7\u05e9\u05d5\u05e8\u05d4\t:ADJECTIVE-F,S,ABS:\t\u05e7\u05e9\u05d5\u05e8\tO\tO\n\u05dc\u05de\u05d7\u05dc\u05d4\tPREPOSITION+DEF:NOUN-F,S,ABS:\t\u05de\u05d7\u05dc\u05d4\tB-NP\tdisease-C0012634\n\u05d0\u05d5\t:CONJUNCTION:\t\u05d0\u05d5\tO\tO\n\u05d0\u05e4\u05d9\u05dc\u05d5\t:CONJUNCTION:\t\u05d0\u05e4\u05d9\u05dc\u05d5\tO\tO\n\u05dc\u05e9\u05d9\u05e0\u05d5\u05d9\tPREPOSITION:NOUN-M,S,ABS:\t\u05e9\u05d9\u05e0\u05d5\u05d9\tB-NP\tdifference-C1705241\n\u05d1\u05de\u05dc\u05d7\u05d9\u05dd\tPREPOSITION:NOUN-M,P,ABS:\t\u05de\u05dc\u05d7\tB-NP\tsalt-C0036140\n\u05d1\u05d3\u05dd\tPREPOSITION:NOUN-M,S,ABS:\t\u05d3\u05dd\tB-NP\tblood-C0005767\n\u05db\u05de\u05d5\t:PREPOSITION:\t\u05db\u05de\u05d5\tO\tO\n\u05de\u05d2\u05e0\u05d6\u05d9\u05d5\u05dd\t:NOUN-M,S,ABS:\t\u05de\u05d2\u05e0\u05d6\u05d9\u05d5\u05dd\tB-NP\tmagnesium-C0024467\n\u05d0\u05d5\t:CONJUNCTION:\t\u05d0\u05d5\tO\tO\n\u05e7\u05dc\u05e6\u05d9\u05d5\u05dd\t:NOUN-M,S,ABS:\t\u05e7\u05dc\u05e6\u05d9\u05d5\u05dd\tB-NP\tcalcium-C0006675\n\u05d0\u05d5\u05ea\u05dd\t:AT_PREP:PRONOMIAL-M,P,3\t\u05d0\u05ea\tB-NP\tO\n\u05db\u05d3\u05d0\u05d9\t:MODAL:\t\u05db\u05d3\u05d0\u05d9\tO\tO\n\u05dc\u05d1\u05d3\u05d5\u05e7\t:VERB-TOINFINITIVE:\t\u05d1\u05d3\u05e7\tO\tO\n\u05de\u05e9\u05d7\u05d4\t:NOUN-F,S,ABS:\t\u05de\u05e9\u05d7\u05d4\tB-NP\tcream-C0700385\n\u05e0\u05d2\u05d3\t:PREPOSITION:\t\u05e0\u05d2\u05d3\tO\tO\n\u05d2\u05d9\u05e8\u05d5\u05d3\u05d9\u05dd\t:NOUN-M,P,ABS:\t\u05d2\u05d9\u05e8\u05d5\u05d3\tB-NP\tO\n\u05d9\u05db\u05d5\u05dc\u05d4\t:MODAL-F,S,A,BEINONI:\t\u05d9\u05db\u05d5\u05dc\tO\tO\n\u05dc\u05d4\u05d9\u05d5\u05ea\t:COPULA-TOINFINITIVE,POSITIVE:\t\u05d4\u05d5\u05d0\tO\tO\n\u05dc\u05de\u05e9\u05dc\t:CONJUNCTION:\t\u05dc\u05de\u05e9\u05dc\tO\tO\n\u05ea\u05e8\u05d9\u05d0\u05d5\u05dc\u05d5\u05df\t:PROPERNAME:\t\u05ea\u05e8\u05d9\u05d0\u05d5\u05dc\u05d5\u05df\tB-NP\tO\n-\t:PUNCUATION:\t-\tI-NP\tO\n\u05e1\u05d5\u05d2\t:NOUN-M,S,ABS:\t\u05e1\u05d5\u05d2\tI-NP\tkind-C0332307\n\u05e9\u05dc\t:SHEL_PREP:\t\u05e9\u05dc\tI-NP\tO\n\u05de\u05e9\u05d7\u05ea\t:NOUN-F,S,CONST:\t\u05de\u05e9\u05d7\u05d4\tI-NP\tcream-C0700385\n\u05e1\u05d8\u05e8\u05d5\u05d0\u05d9\u05d3\u05d9\u05dd\t:NOUN-M,P,ABS:\t\u05e1\u05d8\u05e8\u05d5\u05d0\u05d9\u05d3\tI-NP\tSteroids-C0038317\n\u05d4\u05e6\u05d9\u05e4\u05d5\u05e8\u05e0\u05d9\u05dd\tDEF:NOUN-M,P,ABS:\t\u05e6\u05d9\u05e4\u05d5\u05e8\u05df\tI-NP\tnail-C0027342\n\u05d4\u05db\u05d5\u05d0\u05d1\u05d5\u05ea\tDEF:ADJECTIVE-F,P,ABS:\t\u05db\u05d5\u05d0\u05d1\tI-NP\tO\n\u05d6\u05d5\t:PRONOUN-F,S,3:\t\u05d6\u05d5\tB-NP\tO\n\u05d1\u05e2\u05d9\u05d4\t:NOUN-F,S,ABS:\t\u05d1\u05e2\u05d9\u05d4\tI-NP\tproblem-C0033213\n\u05d5\u05e6\u05e8\u05d9\u05da\tCONJ:MODAL-M,S,A:\t\u05e6\u05e8\u05d9\u05da\tO\tO\n\u05dc\u05d4\u05d6\u05d4\u05e8\t:VERB-TOINFINITIVE:\t\u05e0\u05d6\u05d4\u05e8\tO\tO\n\u05de\u05d6\u05d9\u05d4\u05d5\u05dd\tPREPOSITION:NOUN-M,S,CONST:\t\u05d6\u05d9\u05d4\u05d5\u05dd\tB-NP\tinfection-C0009450\n\u05e4\u05d8\u05e8\u05d9\u05d9\u05ea\u05d9\t:NOUN-F,S,ABS:POSSESIVE-MF,S,1\t\u05e4\u05d8\u05e8\u05d9\u05d9\u05d4\tI-NP\tmushrooms-C0001774\n\u05d0\u05d5\t:CONJUNCTION:\t\u05d0\u05d5\tI-NP\tO\n\u05d7\u05d9\u05d3\u05e7\u05d9\t:NOUN-M,P,CONST:\t\u05d7\u05d9\u05d3\u05e7\u05d9\tI-NP\tO\n\u05d7\u05d5\u05d1\u05d1\t:NOUN-M,S,ABS:\t\u05d7\u05d5\u05d1\u05d1\tO\tO\n\n\n"
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\nTagging from a remote server takes time\n1. Great hackathon project - Hadoop this to tag quickley\n2. let's load the results from canned data\n\"\"\"\ntaggedDocs = {}\ncannedDir = \"sampleTagged\"\nfor f in os.listdir(cannedDir):\n    fin = open(os.path.join(cannedDir,f),\"r\",\"utf8\")\n    taggedDocs[f] = fin.read()\nprint taggedDocs.values()[1]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\u05d0\u05e0\u05d9\t:PRONOUN-MF,S,1:\t\u05d4\u05d5\u05d0\tB-NP\tO\n\u05de\u05d1\u05d9\u05df\t:VERB-M,S,A,BEINONI:\t\u05d4\u05d1\u05d9\u05df\tO\tO\n\u05e9\u05d0\u05ea\tREL-SUBCONJ:AT_PREP:\t\u05d0\u05ea\tB-NP\tO\n\u05e0\u05de\u05e6\u05d0\u05ea\t:VERB-F,S,A,BEINONI:\t\u05e0\u05de\u05e6\u05d0\tO\tO\n\u05d1\u05de\u05e2\u05e7\u05d1\tPREPOSITION:NOUN-M,S,ABS:\t\u05de\u05e2\u05e7\u05d1\tB-NP\tsurveillance-C0220920\n\u05de\u05e1\u05d5\u05d3\u05e8\t:ADJECTIVE-M,S,ABS:\t\u05de\u05e1\u05d5\u05d3\u05e8\tI-NP\tO\n.\t:PUNCUATION:\t.\tO\tO\n\n\u05de\u05e7\u05d5\u05d1\u05dc\t:ADJECTIVE-M,S,ABS:\t\u05de\u05e7\u05d5\u05d1\u05dc\tO\tO\n\u05dc\u05ea\u05ea\t:VERB-TOINFINITIVE:\t\u05e0\u05ea\u05df\tO\tO\n\u05d8\u05d9\u05e4\u05d5\u05dc\t:NOUN-M,S,ABS:\t\u05d8\u05d9\u05e4\u05d5\u05dc\tB-NP\tTherapeutic procedure-C0087111\n\u05d1\u05e7\u05d5\u05de\u05d3\u05d9\u05df\tPREPOSITION:NOUN-M,S,ABS:\t\u05e7\u05d5\u05de\u05d3\u05d9\u05df\tB-NP\twarfarin-C0043031\n\u05dc\u05e4\u05d7\u05d5\u05ea\t:ADVERB:\t\u05dc\u05e4\u05d7\u05d5\u05ea\tI-NP\tO\n3\t:NUMERAL:\t###NUMBER###\tI-NP\tO\n\u05d7'\t:ADJECTIVE-M,S,ABS:\t\u05d7'\tI-NP\tO\n\u05d5\u05dc\u05d1\u05e6\u05e2\tCONJ:VERB-TOINFINITIVE:\t\u05d1\u05d9\u05e6\u05e2\tO\tO\n\u05d1\u05d3\u05d9\u05e7\u05d5\u05ea\t:NOUN-F,P,ABS:\t\u05d1\u05d3\u05d9\u05e7\u05d4\tB-NP\texamination-C0031809\n\u05dc\u05e7\u05e8\u05d9\u05e9\u05d9\u05d5\u05ea\tPREPOSITION:NOUN-F,S,CONST:\t\u05e7\u05e8\u05d9\u05e9\u05d9\u05d5\u05ea\tB-NP\tO\n\u05d9\u05ea\u05e8\t:NOUN-M,S,ABS:\t\u05d9\u05ea\u05e8\tI-NP\tO\n.\t:PUNCUATION:\t.\tO\tO\n\n\u05d1\u05d4\u05ea\u05d0\u05dd\t:ADVERB:\t\u05d1\u05d4\u05ea\u05d0\u05dd\tO\tO\n\u05dc\u05ea\u05d5\u05e6\u05d5\u05ea\tPREPOSITION:NOUN-MF,P,ABS:POSSESIVE-MF,S,1\t\u05ea\u05d5\u05e6\u05d5\tB-NP\tO\n\u05d9\u05e0\u05ea\u05d5\t:NOUN-M,P,CONST:\t\u05d9\u05e0\u05ea\u05d5\tI-NP\tO\n\u05d4\u05e0\u05d7\u05d9\u05d5\u05ea\t:NOUN-F,P,ABS:\t\u05d4\u05e0\u05d7\u05d9\u05d4\tI-NP\tO\n.\t:PUNCUATION:\t.\tO\tO\n\n\u05d2\u05e8\u05d1\t:NOUN-M,S,ABS:\t\u05d2\u05e8\u05d1\tB-NP\tO\n\u05d0\u05dc\u05e1\u05d8\u05d9\t:ADJECTIVE-M,S,ABS:\t\u05d0\u05dc\u05e1\u05d8\u05d9\tI-NP\tO\n\u05dc\u05e4\u05d7\u05d5\u05ea\t:ADVERB:\t\u05dc\u05e4\u05d7\u05d5\u05ea\tO\tO\n\u05e2\u05d3\t:PREPOSITION:\t\u05e2\u05d3\tO\tO\n\u05d9\u05e8\u05d9\u05d3\u05ea\t:NOUN-F,S,CONST:\t\u05d9\u05e8\u05d9\u05d3\u05d4\tB-NP\tDownward-C0205104\n\u05d4\u05e0\u05e4\u05d9\u05d7\u05d5\u05ea\tDEF:NOUN-F,S,ABS:\t\u05e0\u05e4\u05d9\u05d7\u05d5\u05ea\tI-NP\tswelling-C0013604\n,\t:PUNCUATION:\t,\tO\tO\n\u05d1\u05e1\u05d5\u05e3\tPREPOSITION:NOUN-M,S,CONST:\t\u05e1\u05d5\u05e3\tB-NP\tO\n\u05d4\u05d8\u05d9\u05e4\u05d5\u05dc\tDEF:NOUN-M,S,ABS:\t\u05d8\u05d9\u05e4\u05d5\u05dc\tI-NP\tTherapeutic procedure-C0087111\n.\t:PUNCUATION:\t.\tO\tO\n\n\n"
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\ncreate features\n\"\"\"\n\ndef getFeatures(taggedText):\n    features = []\n    for line in taggedText.split(\"\\n\"):\n        if len(line) > 10:\n            data = line.rstrip().split(\"\\t\")\n            if data[1].split(\":\")[1][:3] in [\"VER\",\"NOU\",\"ADJ\",\"FOR\"]:\n                word = data[2]\n                if data[1].split(\":\")[0] == \"DEF\" and word[0].encode(\"utf8\") == \"\u05d4\":\n                    word = word[1:]\n                features.append(word)\n            \n    return features\nprint \"\\n\".join(getFeatures(taggedDocs.values()[1]))",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\u05d4\u05d1\u05d9\u05df\n\u05e0\u05de\u05e6\u05d0\n\u05de\u05e2\u05e7\u05d1\n\u05de\u05e1\u05d5\u05d3\u05e8\n\u05de\u05e7\u05d5\u05d1\u05dc\n\u05e0\u05ea\u05df\n\u05d8\u05d9\u05e4\u05d5\u05dc\n\u05e7\u05d5\u05de\u05d3\u05d9\u05df\n\u05d7'\n\u05d1\u05d9\u05e6\u05e2\n\u05d1\u05d3\u05d9\u05e7\u05d4\n\u05e7\u05e8\u05d9\u05e9\u05d9\u05d5\u05ea\n\u05d9\u05ea\u05e8\n\u05ea\u05d5\u05e6\u05d5\n\u05d9\u05e0\u05ea\u05d5\n\u05d4\u05e0\u05d7\u05d9\u05d4\n\u05d2\u05e8\u05d1\n\u05d0\u05dc\u05e1\u05d8\u05d9\n\u05d9\u05e8\u05d9\u05d3\u05d4\n\u05e0\u05e4\u05d9\u05d7\u05d5\u05ea\n\u05e1\u05d5\u05e3\n\u05d8\u05d9\u05e4\u05d5\u05dc\n"
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "fout = open(\"mydata2.txt\",\"w\",\"utf8\")\nfout.write(\"class\\ttexts\\n\")\n\nfor doc in taggedDocs:\n    mylabel = \"blood\"\n    if \"lung\" in doc: #unicode(doc,encoding=\"cp1255\"):\n        mylabel = \"lung\"\n    \n    fout.write(mylabel+\"\\t\"+\" \".join(getFeatures(taggedDocs[doc]))+\"\\n\")\nfout.close()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\nRead the data into a panda data frame\n\"\"\"\nimport pandas as pd\nimport numpy\ndf = pd.read_csv(\"mydata2.txt\",sep=\"\\t\",header=0,encoding=\"utf\")\ndf = df.dropna()\nnumpy.random.seed(5)\ndf2 = df.reindex(numpy.random.permutation(df.index))\nprint df2",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1375 entries, 1017 to 867\nData columns (total 2 columns):\nclass    1375  non-null values\ntexts    1375  non-null values\ndtypes: object(2)\n"
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": "C:\\Anaconda\\lib\\site-packages\\pandas\\core\\config.py:570: DeprecationWarning: height has been deprecated.\n\n  warnings.warn(d.msg, DeprecationWarning)\n"
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn.feature_extraction.text import TfidfVectorizer\n#Minimal frequency should be much higher for big data (50-100)\nMINIMAL_FREQUENCY = 4 \n# token appearing in more than 10-30% may be an in domain stop word (\"EMC\")\nMAXIMAL_FREQUENCY_FRACTION = 0.3 \n#We have 40 samples, so 40 features should be safe\nNUMBER_OF_FEATURES = 100\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=MAXIMAL_FREQUENCY_FRACTION, min_df = MINIMAL_FREQUENCY,ngram_range = (1,1),max_features=NUMBER_OF_FEATURES)\nX = vectorizer.fit_transform(numpy.asarray(df2.texts)).toarray()\n\nfor w in vectorizer.__dict__[\"vocabulary_\"]:\n    print w",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\u05e7\u05d8\u05df\n\u05d1\u05e8\u05d6\u05dc\n\u05e4\u05e8\u05d8\n\u05d1\u05e2\u05d9\u05d4\n\u05e0\u05d5\u05e1\u05e3\n\u05de\u05e7\u05d5\u05dd\n\u05d3\u05dd\n\u05ea\u05d5\u05e6\u05d0\u05d4\n\u05e2\u05e5\n\u05e1\u05e4\u05d9\u05e8\u05d4\n\u05de\u05e7\u05e8\u05d4\n\u05de\u05d5\u05de\u05d7\u05d4\n\u05d3\u05e8\u05da\n\u05de\u05d8\u05e4\u05dc\n\u05d0\u05dd\n\u05e9\u05dc\u05d1\n\u05d4\u05e8\u05d9\u05d5\u05df\n\u05e9\u05e0\u05d4\n\u05e2\u05e0\u05d4\n\u05d4\u05de\u05d8\u05d5\u05dc\u05d5\u05d2\n\u05de\u05de\u05e6\u05d0\n\u05de\u05e9\u05e4\u05d7\u05d4\n\u05de\u05d9\u05d3\u05d4\n\u05de\u05e2\u05e7\u05d1\n\u05e8\u05d1\n\u05de\u05e1\u05e4\u05e8\n\u05e4\u05e0\u05d4\n\u05de\u05e0\u05d4\n\u05ea\u05e8\u05d5\u05e4\u05d4\n\u05d0\u05d7\u05e8\n\u05e8\u05d9\u05d0\u05d4\n\u05e9\u05e7\u05dc\n\u05d6\u05de\u05df\n\u05db\u05dc\u05dc\n\u05d7\u05d5\u05d1\u05d1\n\u05e6\u05d5\u05e8\u05da\n\u05e7\u05d9\u05d1\u05dc\n\u05e2\u05e8\u05da\n\u05d2\u05d9\u05dc\n\u05e0\u05e8\u05d0\u05d4\n\u05e9\u05dc\u05d5\u05dd\n\u05e8\u05de\u05d4\n\u05d8\u05d5\u05d1\n\u05e1\u05d9\u05d1\u05d4\n\u05d4\u05e6\u05dc\u05d7\u05d4\n\u05ea\u05e7\u05d9\u05df\n\u05ea\u05d0\n\u05d7\u05d6\u05e8\n\u05d7\u05d6\u05d4\n\u05d8\u05d9\u05e4\u05d5\u05dc\n\u05d9\u05d3\u05e2\n\u05e7\u05d1\u05dc\u05d4\n\u05e0\u05ea\u05df\n\u05de\u05e6\u05d1\n\u05e7\u05e9\u05e8\n\u05e4\u05d5\u05e8\u05d5\u05dd\n\u05d1\u05d3\u05e7\n\u05d2\u05d1\u05d5\u05d4\n\u05e7\u05d9\u05d9\u05dd\n\u05e1\u05d9\u05db\u05d5\u05d9\n\u05e1\u05d9\u05db\u05d5\u05df\n\u05e9\u05d0\u05dc\u05d4\n\u05e0\u05d9\u05ea\u05d5\u05d7\n\u05e2\u05e9\u05d4\n\u05e1\u05e8\u05d8\u05df\n\u05e8\u05d0\u05d4\n\u05e2\u05e6\u05dd\n\u05e9\u05d5\u05e0\u05d4\n\u05e8\u05d1\u05d9\n\u05d0\u05d5\u05e4\u05df\n\u05d1\u05d9\u05e6\u05e2\n\u05d7\u05e9\u05d5\u05d1\n\u05d7\u05d5\u05dc\u05d4\n\u05e2\u05d1\u05e8\n\u05d4\u05e6\u05d9\u05e2\n\u05d3\u05d1\u05e8\n\u05e0\u05de\u05d5\u05da\n\u05d4\u05dc\u05da\n\u05de\u05ea\u05d0\u05d9\u05dd\n\u05d4\u05e0\u05d9\u05d7\n\u05e0\u05d5\u05e9\u05d0\n\u05de\u05d7\u05dc\u05d4\n\u05d4\u05de\u05dc\u05d9\u05e5\n\u05d1\u05d9\u05e8\u05d5\u05e8\n\u05d2\u05e8\u05dd\n\u05d4\u05ea\u05d9\u05d9\u05e2\u05e5\n\u05e1\u05d5\u05d2\n\u05d9\u05ea\u05e8\n\u05e7\u05e8\u05d9\u05e9\u05d4\n\u05d0\u05d9\u05e9\n\u05e0\u05d1\u05d3\u05e7\n\u05d8\u05d9\u05e4\u05dc\n\u05ea\u05e9\u05d5\u05d1\u05d4\n\u05ea\u05d5\u05e4\u05e2\u05d4\n\u05d1\u05d3\u05d9\u05e7\u05d4\n\u05d1\u05e8\u05d5\u05e8\n\u05d3\u05d5\u05d1\u05e8\n\u05e4\u05e8\u05d5\u05e3\n\u05d4\u05de\u05e9\u05da\n\u05d2\u05d9\u05d3\u05d5\u05dc\n"
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "TEST_SIZE = 500\nY= numpy.asarray(df2[\"class\"])\ntrain_x = X[:-TEST_SIZE]\ntrain_y = list(Y[:-TEST_SIZE])\ntest_x  = X[-TEST_SIZE:]\ntest_y  = list(Y[-TEST_SIZE:])\n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn.metrics import f1_score,classification_report\nfrom sklearn.naive_bayes import MultinomialNB\ndef trainTestModel(train_x,train_y,test_x,test_y,model):\n    print \"training\"\n    model.fit(train_x, train_y)\n    print \"test\"\n    pred = model.predict(test_x)\n    try:\n        print classification_report(test_y, pred)\n    except:\n        print \"Error\"\n    return True\nclf = MultinomialNB()\ntrainTestModel(train_x,train_y,test_x,test_y,clf)\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "training\ntest\n             precision    recall  f1-score   support\n\n      blood       0.92      0.95      0.94       319\n       lung       0.91      0.85      0.88       181\n\navg / total       0.92      0.92      0.92       500\n\n"
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 151,
       "text": "True"
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\"\"\"\nEvaluate the model qualitatively\nAre the features really robust?\n\"\"\"\nfnames = list(numpy.asarray(vectorizer.get_feature_names()))\nfeatures = []\nfor sclass,weights in zip(clf.classes_,clf.coef_):\n    features += [(abs(weight),feat) for weight,feat in zip(weights,fnames)]\nfeatures.sort(reverse=True)\nfor weight,word in features[:50]:\n    print word",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\u05e7\u05e8\u05d9\u05e9\u05d4\n\u05d4\u05de\u05d8\u05d5\u05dc\u05d5\u05d2\n\u05e1\u05e4\u05d9\u05e8\u05d4\n\u05e8\u05d1\u05d9\n\u05d9\u05ea\u05e8\n\u05e2\u05e8\u05da\n\u05e4\u05e8\u05d5\u05e3\n\u05d1\u05e8\u05d6\u05dc\n\u05e2\u05e5\n\u05d4\u05e8\u05d9\u05d5\u05df\n\u05ea\u05e7\u05d9\u05df\n\u05e8\u05de\u05d4\n\u05e0\u05de\u05d5\u05da\n\u05de\u05e2\u05e7\u05d1\n\u05d3\u05dd\n\u05db\u05dc\u05dc\n\u05de\u05e7\u05d5\u05dd\n\u05d7\u05d6\u05e8\n\u05d2\u05e8\u05dd\n\u05e7\u05d1\u05dc\u05d4\n\u05d3\u05e8\u05da\n\u05e2\u05d1\u05e8\n\u05de\u05ea\u05d0\u05d9\u05dd\n\u05e6\u05d5\u05e8\u05da\n\u05e2\u05e6\u05dd\n\u05d4\u05e0\u05d9\u05d7\n\u05e4\u05e8\u05d8\n\u05d1\u05e8\u05d5\u05e8\n\u05ea\u05d5\u05e4\u05e2\u05d4\n\u05d1\u05d9\u05e8\u05d5\u05e8\n\u05d0\u05d9\u05e9\n\u05d2\u05d1\u05d5\u05d4\n\u05e9\u05e0\u05d4\n\u05de\u05e9\u05e4\u05d7\u05d4\n\u05d4\u05e6\u05d9\u05e2\n\u05d4\u05ea\u05d9\u05d9\u05e2\u05e5\n\u05e7\u05e9\u05e8\n\u05d0\u05dd\n\u05e7\u05d9\u05d1\u05dc\n\u05e1\u05d9\u05db\u05d5\u05df\n\u05d4\u05de\u05e9\u05da\n\u05d7\u05e9\u05d5\u05d1\n\u05e1\u05d9\u05d1\u05d4\n\u05de\u05d5\u05de\u05d7\u05d4\n\u05de\u05d9\u05d3\u05d4\n\u05e0\u05ea\u05df\n\u05e4\u05e0\u05d4\n\u05ea\u05d0\n\u05e9\u05dc\u05d1\n\u05e2\u05e0\u05d4\n"
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}